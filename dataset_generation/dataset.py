from time import sleep
from typing import List, Optional
from pathlib import Path
from json import loads, dumps
from threading import Thread
from array import array
import numpy as np
import pyaudio
from tqdm import tqdm
from tdw.tdw_utils import AudioUtils, TDWUtils
from tdw.py_impact import PyImpact
from tdw.output_data import Rigidbodies
from magnebot import Magnebot, ArmJoint
from magnebot.scene_state import SceneState
from magnebot.util import get_data
from multimodal_challenge.multimodal_base import MultiModalBase
from multimodal_challenge.trial import Trial
from multimodal_challenge.encoder import Encoder
from multimodal_challenge.paths import REHEARSAL_DIRECTORY, ENV_AUDIO_MATERIALS_PATH, OBJECT_INIT_DIRECTORY, \
    DATASET_DIRECTORY
from multimodal_challenge.util import get_object_init_commands, NUM_LAYOUTS
from multimodal_challenge.multimodal_object_init_data import MultiModalObjectInitData
from multimodal_challenge.dataset.dataset_trial import DatasetTrial
from multimodal_challenge.dataset.env_audio_materials import EnvAudioMaterials
from multimodal_challenge.magnebot_init_data import MagnebotInitData


class Dataset(MultiModalBase):
    """
    Use the initialization data generated by [`rehearsal.py`](rehearsal.md) to create [`Trials`](../api/trial.md). A `Trial` is initialization data for each object in the scene (position, rotation, etc.), initialization data for the Magenbot, and a .wav file.

    # Requirements

    - The `multimodal_challenge` Python module
    - Optional: Run [`rehearsal.py`](rehearsal.md) to generate the initialization data (there is already cached data in the Python module)
    - Audio drivers
    - [`PyAudio`](https://people.csail.mit.edu/hubert/pyaudio/) If you're using Windows and Python 3.7 or later, use a wheel from [this site](https://www.lfd.uci.edu/~gohlke/pythonlibs/) and install it via: `pip3 install path/to/the/downloaded.whl` (replace this with the actual path to the downloaded file)

    # Usage

    1. `cd dataset`
    2. `python3 dataset.py`
    3. Run build

    **This is a VERY long process.**

    # How it works

    **Per scene_layout combination:**

    1. Load the corresponding object init data and the [`DatasetTrial`](../api/dataset_trial.md) data from rehearsal.py

    **Per trial:**

    1. Re-initialize the scene.
    2. Select the next `DatasetTrial` initialization object in the list.
    3. Add a Magnebot. Turn the Magnebot away from `DatasetTrial.position`. Set random position and rotation parameters.
    4. Add the target object from the `DatasetTrial` parameters (position, force, etc.)
    5. Initialize audio in the scene and audio recording.
    6. Let the object fall. Use PyImpact to generate collisions.
    7. The trial stops either when the sound stops playing or if a maximum number of frames has been reached.
    8. Save the results as a `Trial` object in a .json file.

    **Result:** A directory of `Trial` objects per scene_layout combination:

    ```
    D:/multimodal_challenge/dataset  # See dataset in config.ini
    ....1_0/  # scene_layout
    ........0.json
    ........1.json
    ```
    """

    """:class_var
    The PyAudio object. This is used to determine when a trial ends (when the audio stops playing).
    """
    PY_AUDIO: pyaudio.PyAudio = pyaudio.PyAudio()
    """:class_var
    True if there is currently audio playing. Don't set this value manually! It is handled in a separate thread.
    """
    AUDIO_IS_PLAYING: bool = False
    """:class_var
    If True, continue to listen to audio. Use this to stop the PyAudio thread.
    """
    LISTEN_TO_AUDIO: bool = False
    """:class_var
    The PyImpact object used to generate impact sound audio at runtime.
    """
    PY_IMPACT: PyImpact = PyImpact()
    """:class_var
    The path to the temporary audio file.
    """
    TEMP_AUDIO_PATH: Path = DATASET_DIRECTORY.joinpath("temp.wav")

    def __init__(self, port: int = 1071, random_seed: int = 0):
        """
        Create the network socket and bind the socket to the port.

        :param port: The port number.
        :param random_seed: The seed for the random number generator.
        """

        super().__init__(port=port, random_seed=random_seed, screen_height=128, screen_width=128, skip_frames=0)
        self.communicate([{"$type": "set_render_quality",
                           "render_quality": 0},
                          {"$type": "set_target_framerate",
                           "framerate": 60}])
        """:field
        The name of the next trial.
        """
        self.trial_count: int = 0
        """:field
        The name of the scene in the current trial.
        """
        self.scene: str = ""
        """:field
        The name of the layout of the current trial.
        """
        self.layout: int = -1
        """:field
        Parameters to define each trial. See: `rehearsal.py`.
        """
        self.trials: List[DatasetTrial] = list()
        """:field
        The ID of the target object in the current trial.
        """
        self.target_object_id: int = -1
        """:field
        The PyImpact audio materials used for the environment as an `EnvAudioMaterials` object.
        """
        self.env_audio_materials: Optional[EnvAudioMaterials] = None
        """:field
        A dummy object ID for the environment. This is reassigned per trial.
        """
        self.env_id: int = -1
        # Magnebot initialization data.
        self._magnebot_init_data: Optional[MagnebotInitData] = None
        # The initial position of the Magnebot for this trial.
        self._magnebot_position: Optional[np.array] = None

    def run(self) -> None:
        """
        Generate the entire dataset for each scene_layout combination.
        """

        for f in OBJECT_INIT_DIRECTORY.iterdir():
            # Expected: mm_kitchen_1a_0.json, mm_kitchen_1a_1.json, ... , mm_kitchen_2b_2.json, ...
            if f.is_file() and f.suffix == ".json" and f.name.endswith("_0.json"):
                for i in range(NUM_LAYOUTS):
                    self.do_trials(scene=f.name.replace(".json", "")[:-2], layout=str(i))

    def do_trials(self, scene: str, layout: str) -> None:
        """
        Get the cached trial initialization data for a scene_layout combination and do each trial.
        This will try to avoid overwriting existing trial results.
        This will start a thread to listen to audio on the sound card to determine if a trial is done.

        :param scene: The name of the scene.
        :param layout: The index of the furniture layout.
        """

        # Remember the name of the scene.
        self.scene = scene
        self.layout = int(layout)
        output_directory = DATASET_DIRECTORY.joinpath(f"{scene}_{layout}")

        # Get the environment audio materials.
        data = loads(ENV_AUDIO_MATERIALS_PATH)
        self.env_audio_materials = EnvAudioMaterials(**data[scene])
        self.trial_count: int = 0
        # Get the last trial number, to prevent overwriting files.
        for f in output_directory.iterdir():
            if f.is_file() and f.suffix == ".json":
                tc = int(f.name.replace(".json", ""))
                if tc > self.trial_count:
                    self.trial_count = tc + 1
        # We already completed this portion of the dataset.
        if self.trial_count == len(self.trials):
            return
        # Load the cached trial data.
        self.trials = [DatasetTrial(**d) for d in
                       loads(REHEARSAL_DIRECTORY.joinpath(f"{scene}_{layout}.json").read_text(encoding="utf-8"))]
        # Create a progress bar.
        pbar = tqdm(total=len(self.trials))
        pbar.update(self.trial_count)
        # Start listening to audio on a separate thread.
        t = Thread(target=Dataset._listen_for_audio)
        t.daemon = True
        try:
            t.start()
            # Initialize the scene and do the trial.
            for i in range(self.trial_count, len(self.trials)):
                pbar.set_description(f"{scene}_{layout} {i}")
                self.do_trial(output_directory=output_directory)
                pbar.update(1)
        # Close the audio thread, stop fmedia, and stop the progress bar.
        finally:
            Dataset.LISTEN_TO_AUDIO = False
            AudioUtils.stop()
            pbar.close()

    def do_trial(self, output_directory: Path) -> None:
        """
        Initialize the scene. This will add the target (dropped) object, the scene objects, and the Magnebot,
        as well as set a position, rotation, torso height, column rotation, and camera angles for the Magnebot.

        Start recording audio and let the object fall. The simulation ends when there's no more audio or
        if the simulation continued for too long.

        :param output_directory: The output directory for the trial data.
        """

        self.init_scene(scene=self.scene, layout=self.layout)
        # Get the PyImpact audio materials for the floor and walls.
        floor = EnvAudioMaterials.RESONANCE_AUDIO_TO_PY_IMPACT[self.env_audio_materials.floor]
        wall = EnvAudioMaterials.RESONANCE_AUDIO_TO_PY_IMPACT[self.env_audio_materials.wall]
        # Cache the names of each object in PyImpact.
        object_names = dict()
        for object_id in self.objects_static:
            object_names[object_id] = self.objects_static[object_id].name
        Dataset.PY_IMPACT.set_default_audio_info(objects=object_names)
        try:
            # Start recording the audio.
            AudioUtils.start(output_path=Dataset.TEMP_AUDIO_PATH)
            # These commands must be sent here because `init_scene()` will try to make the Magnebot moveable.
            self._next_frame_commands.extend([{"$type": "send_rigidbodies",
                                               "frequency": "always"},
                                              {"$type": "set_immovable",
                                               "immovable": True},
                                              {"$type": "enable_image_sensor",
                                               "enable": False}])
            frame: int = 0
            done: bool = False
            resp = self.communicate([])
            # Let the simulation run until there's too many frames or if there's no audio.
            while not done and frame < 1000:
                # Get impact sound commands.
                commands = Dataset.PY_IMPACT.get_audio_commands(resp=resp, floor=floor, wall=wall, resonance_audio=True)
                # Check if the object stopped moving (there won't be audio or collisions while it's falling).
                rigidbodies = get_data(resp=resp, d_type=Rigidbodies)
                sleeping = False
                for i in range(rigidbodies.get_num()):
                    if rigidbodies.get_id(i) == self.target_object_id:
                        sleeping = rigidbodies.get_sleeping(i)
                        break
                # This trial is done if the object isn't moving, there's no audio playing, and no pending collisions.
                if sleeping and not Dataset.AUDIO_IS_PLAYING and len(commands) == 0:
                    done = True
                else:
                    resp = self.communicate(commands)
                frame += 1
        finally:
            AudioUtils.stop()

        # Convert the current state of each object to initialization data.
        state = SceneState(resp=self.communicate([]))
        object_init_data: List[MultiModalObjectInitData] = list()
        for o_id in self.objects_static:
            name = self.objects_static[o_id].name
            o = MultiModalObjectInitData(name=name,
                                         position=TDWUtils.array_to_vector3(state.object_transforms[o_id].position),
                                         rotation=TDWUtils.array_to_vector4(state.object_transforms[o_id].rotation),
                                         kinematic=self.objects_static[o_id].kinematic,
                                         scale_factor={"x": 1, "y": 1, "z": 1})
            object_init_data.append(o)

        # Cache the result of the trial.
        ci = Trial(scene=self.scene,
                   magnebot=self._magnebot_init_data,
                   audio=Dataset.TEMP_AUDIO_PATH.read_bytes(),
                   target_object=self.target_object_id,
                   object_init_data=object_init_data)
        # Remove the temp file.
        Dataset.TEMP_AUDIO_PATH.unlink()
        # Write the result to disk.
        output_directory.joinpath(f"{self.trial_count}.json").write_text(dumps(ci.__dict__, cls=Encoder),
                                                                         encoding="utf-8")
        self.trial_count += 1

    def _cache_static_data(self, resp: List[bytes]) -> None:
        super()._cache_static_data(resp=resp)
        num_attempts = 0

        # Get a unique env ID.
        got_env_id = False
        while not got_env_id and num_attempts < 100:
            self.env_id = self.get_unique_id()
            got_env_id = self.env_id in self.objects_static
            num_attempts += 1
        assert got_env_id, f"Failed to get an environment ID for PyImpact (this should never happen!)"

    def _get_object_init_commands(self) -> List[dict]:
        # Get the commands for the scene objects.
        commands = get_object_init_commands(scene=self.scene, layout=self.layout)
        return commands

    def _get_start_trial_commands(self) -> List[dict]:
        # Disable any graphics settings that could affect performance (because the camera is off). Set the reverb space.
        return [{"$type": "set_post_process",
                 "value": False},
                {"$type": "enable_reflection_probes",
                 "enable": False},
                {"$type": "set_reverb_space_simple",
                 "env_id": -1,
                 "reverb_floor_material": self.env_audio_materials.floor,
                 "reverb_ceiling_material": self.env_audio_materials.wall,
                 "reverb_front_wall_material": self.env_audio_materials.wall,
                 "reverb_back_wall_material": self.env_audio_materials.wall,
                 "reverb_left_wall_material": self.env_audio_materials.wall,
                 "reverb_right_wall_material": self.env_audio_materials.wall}]

    def _get_end_init_commands(self) -> List[dict]:
        # Add an audio sensor. Apply a force.
        return [{"$type": "add_environ_audio_sensor"},
                {"$type": "apply_force_to_object",
                 "id": self.target_object_id,
                 "force": self.trials[self.trial_count].force}]

    def _set_initial_pose(self) -> None:
        good = False
        num_attempts: int = 0
        joint_ids = [self.magnebot_static.arm_joints[ArmJoint.torso], self.magnebot_static.arm_joints[ArmJoint.column]]
        while not good and num_attempts < 100:
            # Get a random joint positions.
            column_angle = self._rng.uniform(-160, 160)
            torso_height = self._rng.random()
            # Set random camera angles.
            camera_pitch = self._rng.uniform(-Magnebot.CAMERA_RPY_CONSTRAINTS[1] / 2,
                                             Magnebot.CAMERA_RPY_CONSTRAINTS[1] / 2)
            camera_yaw = self._rng.uniform(-Magnebot.CAMERA_RPY_CONSTRAINTS[2] / 2,
                                           Magnebot.CAMERA_RPY_CONSTRAINTS[2] / 2)
            self._magnebot_init_data = MagnebotInitData(position=self._magnebot_position,
                                                        torso_height=torso_height,
                                                        column_angle=column_angle,
                                                        camera_yaw=camera_yaw,
                                                        camera_pitch=camera_pitch)
            # Converts the init data to commands.
            self._next_frame_commands.extend(self._get_magnebot_init_commands(init=self._magnebot_init_data))
            # Strike a cool pose.
            self._do_arm_motion(joint_ids=joint_ids)
            # Update the state.
            self._end_action()
            # If the object isn't visible, this is a good pose.
            good = self.target_object_id not in self.get_visible_objects()
            num_attempts += 1

    def _get_magnebot_position(self) -> np.array:
        # Get all free occupancy map positions.
        occupancy_positions: List[np.array] = list()
        target_object_position = TDWUtils.vector3_to_array(self.trials[self.trial_count].init_data.position)
        for ix, iy in np.ndindex(self.occupancy_map.shape):
            if self.occupancy_map[ix][iy] != 0:
                continue
            px, pz = self.get_occupancy_position(ix, iy)
            occupancy_positions.append(np.array([px, 0, pz]))
        # Sort the occupancy map positions by distance to the target object.
        occupancy_positions = list(sorted(occupancy_positions,
                                          key=lambda p: np.linalg.norm(p - target_object_position)))
        # Get the latter half of the positions (the further positions).
        occupancy_positions = occupancy_positions[int(len(occupancy_positions) / 2.0):]
        # Pick a random position.
        self._magnebot_position = self._rng.choice(occupancy_positions)
        return self._magnebot_position

    def _get_target_object(self) -> Optional[MultiModalObjectInitData]:
        return self.trials[self.trial_count].init_data

    @staticmethod
    def _get_pyaudio_device_index() -> int:
        """
        Source: https://stackoverflow.com/questions/36894315/how-to-select-a-specific-input-device-with-pyaudio

        :return: The index of the system audio device in PyAudio.
        """

        info = Dataset.PY_AUDIO.get_host_api_info_by_index(0)
        num_devices = info.get('deviceCount')
        for i in range(0, num_devices):
            if Dataset.PY_AUDIO.get_device_info_by_host_api_device_index(0, i).get('maxInputChannels') > 0:
                device_name = Dataset.PY_AUDIO.get_device_info_by_host_api_device_index(0, i).get('name')
                if "Stereo Mix" in device_name:
                    return i
        raise Exception("Couldn't find a suitable audio device!")

    @staticmethod
    def _listen_for_audio():
        """
        Source: https://stackoverflow.com/questions/892199/detect-record-audio-in-python

        Start this process in a thread to listen to the audio to determine if anything is still playing.
        """

        device_index: int = Dataset._get_pyaudio_device_index()
        threshold = 5
        chunk_size = 1024
        audio_format = pyaudio.paInt16
        rate = 44100
        stream = Dataset.PY_AUDIO.open(format=audio_format, channels=1, rate=rate,
                                       input=True, output=True,
                                       frames_per_buffer=chunk_size,
                                       input_device_index=device_index)
        Dataset.LISTEN_TO_AUDIO = True
        try:
            # Periodically check to see if any audio is playing.
            while Dataset.LISTEN_TO_AUDIO:
                snd_data = array('h', stream.read(chunk_size))
                Dataset.AUDIO_IS_PLAYING = max(snd_data) > threshold
                sleep(0.1)
        finally:
            stream.stop_stream()
            stream.close()


if __name__ == "__main__":
    dataset_generator = Dataset()
    dataset_generator.run()
