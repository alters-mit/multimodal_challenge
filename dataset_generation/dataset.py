from time import sleep
from typing import List, Optional
from pathlib import Path
from json import loads, dumps
from array import array
import numpy as np
import pyaudio
from tqdm import tqdm
from scipy.signal import convolve2d
from tdw.tdw_utils import AudioUtils, TDWUtils, QuaternionUtils
from tdw.py_impact import PyImpact, ObjectInfo, AudioMaterial
from tdw.output_data import Rigidbodies, Transforms, AudioSources
from magnebot import ActionStatus
from magnebot.scene_state import SceneState
from magnebot.util import get_data
from multimodal_challenge.multimodal_base import MultiModalBase
from multimodal_challenge.trial import Trial
from multimodal_challenge.encoder import Encoder
from multimodal_challenge.paths import REHEARSAL_DIRECTORY, ENV_AUDIO_MATERIALS_PATH, OBJECT_INIT_DIRECTORY, \
    DATASET_DIRECTORY
from multimodal_challenge.util import get_object_init_commands
from multimodal_challenge.multimodal_object_init_data import MultiModalObjectInitData
from multimodal_challenge.dataset.dataset_trial import DatasetTrial
from multimodal_challenge.dataset.env_audio_materials import EnvAudioMaterials
from multimodal_challenge.magnebot_init_data import MagnebotInitData


class Dataset(MultiModalBase):
    """
    Use the initialization data generated by [`rehearsal.py`](rehearsal.md) to create [`Trials`](../api/trial.md). A `Trial` is initialization data for each object in the scene (position, rotation, etc.), initialization data for the Magenbot, and a .wav file.

    # Requirements

    - The `multimodal_challenge` Python module
    - Optional: Run [`rehearsal.py`](rehearsal.md) to generate the initialization data (there is already cached data in the Python module)
    - Audio drivers
    - [`PyAudio`](https://people.csail.mit.edu/hubert/pyaudio/) If you're using Windows and Python 3.7 or later, use a wheel from [this site](https://www.lfd.uci.edu/~gohlke/pythonlibs/) and install it via: `pip3 install path/to/the/downloaded.whl` (replace this with the actual path to the downloaded file)

    # Usage

    1. `cd dataset`
    2. `python3 dataset.py`
    3. Run build

    **This is a VERY long process.**

    # How it works

    **Per scene_layout combination:**

    1. Load the corresponding object init data and the [`DatasetTrial`](../api/dataset_trial.md) data from rehearsal.py

    **Per trial:**

    1. Re-initialize the scene.
    2. Select the next `DatasetTrial` initialization object in the list.
    3. Add a Magnebot. Turn the Magnebot away from `DatasetTrial.position`. Set random position and rotation parameters.
    4. Add the target object from the `DatasetTrial` parameters (position, force, etc.)
    5. Initialize audio in the scene and audio recording.
    6. Let the object fall. Use PyImpact to generate collisions.
    7. The trial stops either when the sound stops playing or if a maximum number of frames has been reached.
    8. Save the results as a `Trial` object in a .json file.

    **Result:** A directory of `Trial` objects per scene_layout combination:

    ```
    D:/multimodal_challenge/dataset  # See dataset in config.ini
    ....1_0/  # scene_layout
    ........0.json
    ........1.json
    ```
    """

    """:class_var
    The PyAudio object. This is used to determine when a trial ends (when the audio stops playing).
    """
    PY_AUDIO: pyaudio.PyAudio = pyaudio.PyAudio()
    """:class_var
    PyImpact initial amp value.
    """
    INITIAL_AMP: float = 0.5
    """:class_var
    The PyImpact object used to generate impact sound audio at runtime.
    """
    PY_IMPACT: PyImpact = PyImpact(initial_amp=INITIAL_AMP)
    """:class_var
    The path to the temporary audio file.
    """
    TEMP_AUDIO_PATH: Path = DATASET_DIRECTORY.joinpath("temp.wav")

    def __init__(self, port: int = 1071, random_seed: int = 0):
        """
        Create the network socket and bind the socket to the port.

        :param port: The port number.
        :param random_seed: The seed for the random number generator.
        """

        super().__init__(port=port, random_seed=random_seed, screen_height=128, screen_width=128, skip_frames=0)
        self.communicate([{"$type": "set_render_quality",
                           "render_quality": 0},
                          {"$type": "set_target_framerate",
                           "framerate": 100}])
        """:field
        The name of the next trial.
        """
        self.trial_count: int = 0
        """:field
        The name of the scene in the current trial.
        """
        self.scene: str = ""
        """:field
        The name of the layout of the current trial.
        """
        self.layout: int = -1
        """:field
        Parameters to define each trial. See: `rehearsal.py`.
        """
        self.trials: List[DatasetTrial] = list()
        """:field
        The ID of the target object in the current trial.
        """
        self.target_object_id: int = -1
        """:field
        The PyImpact audio materials used for the environment as an `EnvAudioMaterials` object.
        """
        self.env_audio_materials: Optional[EnvAudioMaterials] = None
        """:field
        A dummy object ID for the environment. This is reassigned per trial.
        """
        self.env_id: int = -1
        # Magnebot initialization data.
        self._magnebot_init_data: Optional[MagnebotInitData] = None
        # The initial position of the Magnebot for this trial.
        self._magnebot_position: Optional[np.array] = None
        # The PyAudio device index.
        self._device_index: int = Dataset._get_pyaudio_device_index()

    def run(self) -> None:
        """
        Generate the entire dataset for each scene_layout combination.
        """

        for f in OBJECT_INIT_DIRECTORY.iterdir():
            # Expected: mm_kitchen_1a_0.json, mm_kitchen_1a_1.json, ... , mm_kitchen_2b_2.json, ...
            if f.is_file() and f.suffix == ".json" and f.name.endswith("_0.json"):
                self.do_trials(scene=f.name.replace(".json", "")[:-2], layout=f.name[-6])

    def do_trials(self, scene: str, layout: str) -> None:
        """
        Get the cached trial initialization data for a scene_layout combination and do each trial.
        This will try to avoid overwriting existing trial results.
        This will start a thread to listen to audio on the sound card to determine if a trial is done.

        :param scene: The name of the scene.
        :param layout: The index of the furniture layout.
        """

        self._start_action()
        # Remember the name of the scene.
        self.scene = scene
        self.layout = int(layout)
        output_directory = DATASET_DIRECTORY.joinpath(f"{scene}_{layout}")
        if not output_directory.exists():
            output_directory.mkdir(parents=True)

        # Get the environment audio materials.
        data = loads(ENV_AUDIO_MATERIALS_PATH.read_text(encoding="utf-8"))
        self.env_audio_materials = EnvAudioMaterials(**data[scene])
        self.trial_count: int = 0
        # Get the last trial number, to prevent overwriting files.
        for f in output_directory.iterdir():
            if f.is_file() and f.suffix == ".json":
                tc = int(f.name.replace(".json", ""))
                if tc > self.trial_count:
                    self.trial_count = tc + 1
        # Load the cached trial data.
        self.trials = [DatasetTrial(**d) for d in
                       loads(REHEARSAL_DIRECTORY.joinpath(f"{scene}_{layout}.json").read_text(encoding="utf-8"))]
        # We already completed this portion of the dataset.
        if self.trial_count == len(self.trials):
            return
        # Create a progress bar.
        pbar = tqdm(total=len(self.trials))
        pbar.update(self.trial_count)
        try:
            # Initialize the scene and do the trial.
            for i in range(self.trial_count, len(self.trials)):
                pbar.set_description(f"{scene}_{layout} {i}")
                self.do_trial(output_directory=output_directory)
                pbar.update(1)
        # Close the audio thread, stop fmedia, and stop the progress bar.
        finally:
            AudioUtils.stop()
            pbar.close()

    def do_trial(self, output_directory: Path) -> None:
        """
        Initialize the scene. This will add the target (dropped) object, the scene objects, and the Magnebot,
        as well as set a position, rotation, torso height, column rotation, and camera angles for the Magnebot.

        Start recording audio and let the object fall. The simulation ends when there's no more audio or
        if the simulation continued for too long.

        :param output_directory: The output directory for the trial data.
        """

        self.init_scene(scene=self.scene, layout=self.layout)
        # Get the PyImpact audio materials for the floor and walls.
        floor = EnvAudioMaterials.RESONANCE_AUDIO_TO_PY_IMPACT[self.env_audio_materials.floor]
        wall = EnvAudioMaterials.RESONANCE_AUDIO_TO_PY_IMPACT[self.env_audio_materials.wall]
        # Cache the names of each object in PyImpact.
        object_names = dict()
        for object_id in self.objects_static:
            object_names[object_id] = self.objects_static[object_id].name
        for j in self.magnebot_static.joints:
            object_names[j] = self.magnebot_static.joints[j].name
        Dataset.PY_IMPACT.set_default_audio_info(object_names=object_names)
        # Assign audio properties per joint.
        for j in self.magnebot_static.joints:
            Dataset.PY_IMPACT.object_info[self.magnebot_static.joints[j].name] = ObjectInfo(
                name=self.magnebot_static.joints[j].name,
                mass=self.magnebot_static.joints[j].mass,
                amp=0.05,
                resonance=0.65,
                material=AudioMaterial.metal,
                bounciness=0.6,
                library="")
        try:
            # Start recording the audio.
            AudioUtils.start(output_path=Dataset.TEMP_AUDIO_PATH)
            # These commands must be sent here because `init_scene()` will try to make the Magnebot movable.
            # Also, we need some extra output data to handle audio recording.
            resp = self.communicate([{"$type": "send_rigidbodies",
                                      "frequency": "always"},
                                     {"$type": "set_immovable",
                                      "immovable": True},
                                     {"$type": "enable_image_sensor",
                                      "enable": False},
                                     {"$type": "send_audio_sources",
                                      "frequency": "always"}])
            done: bool = False
            # Let the simulation run until there's too many frames or if there's no audio.
            while not done:
                # Get impact sound commands.
                commands = Dataset.PY_IMPACT.get_audio_commands(resp=resp, floor=floor, wall=wall, resonance_audio=True)
                # Check if the object stopped moving (there won't be audio or collisions while it's falling).
                rigidbodies = get_data(resp=resp, d_type=Rigidbodies)
                sleeping = False
                for i in range(rigidbodies.get_num()):
                    if rigidbodies.get_id(i) == self.target_object_id:
                        sleeping = rigidbodies.get_sleeping(i)
                        break
                transforms = get_data(resp=resp, d_type=Transforms)
                # Stop if the object somehow fell below the floor.
                below_floor = False
                for i in range(transforms.get_num()):
                    if transforms.get_id(i) == self.target_object_id:
                        below_floor = transforms.get_position(i)[1] < -1
                        break
                audio = get_data(resp=resp, d_type=AudioSources)
                audio_playing = False
                for i in range(audio.get_num()):
                    if audio.get_is_playing(i):
                        audio_playing = True
                        break
                # This trial is done if the object isn't moving, there's no audio playing, and no pending collisions.
                if below_floor or (sleeping and not audio_playing and len(commands) == 0):
                    done = True
                else:
                    resp = self.communicate(commands)
            # Resonance Audio might continue generating reverb after the AudioSource finishes.
            # So we'll listen to the system audio until we can't hear anything.
            self._listen_for_audio()
        finally:
            AudioUtils.stop()

        # Convert the current state of each object to initialization data.
        state = SceneState(resp=self.communicate([]))
        object_init_data: List[MultiModalObjectInitData] = list()
        target_object_index: int = -1
        index: int = 0
        for o_id in self.objects_static:
            # Get the target object's index in the list.
            if o_id == self.target_object_id:
                target_object_index = index
            index += 1
            name = self.objects_static[o_id].name
            o = MultiModalObjectInitData(name=name,
                                         position=TDWUtils.array_to_vector3(state.object_transforms[o_id].position),
                                         rotation=TDWUtils.array_to_vector4(state.object_transforms[o_id].rotation),
                                         kinematic=self.objects_static[o_id].kinematic,
                                         scale_factor={"x": 1, "y": 1, "z": 1})
            object_init_data.append(o)

        # Cache the result of the trial.
        ci = Trial(scene=self.scene,
                   magnebot=self._magnebot_init_data,
                   audio=Dataset.TEMP_AUDIO_PATH.read_bytes(),
                   target_object_index=target_object_index,
                   object_init_data=object_init_data)
        # Remove the temp file.
        Dataset.TEMP_AUDIO_PATH.unlink()
        # Write the result to disk.
        output_directory.joinpath(f"{self.trial_count}.json").write_text(dumps(ci.__dict__, cls=Encoder),
                                                                         encoding="utf-8")
        self.trial_count += 1

    def init_scene(self, scene: str, layout: int, room: int = None) -> ActionStatus:
        """
        Initialize the scene. Turn the Magnebot away from the object. Let the object fall.

        :param scene: The name of the scene.
        :param layout: The layout index.
        :param room: This parameter is ignored.

        :return: An `ActionStatus` (always success).
        """

        super().init_scene(scene=scene, layout=layout, room=room)
        # Get the angle to the object.
        angle = TDWUtils.get_angle_between(v1=self.state.magnebot_transform.forward,
                                           v2=TDWUtils.vector3_to_array(self.trials[self.trial_count].position) -
                                              self.state.magnebot_transform.position)
        if np.abs(angle) > 180:
            if angle > 0:
                angle -= 360
            else:
                angle += 360
        # Flip the angle and add some randomness. Then turn by the angle to look away from the object.
        angle += 180 + self._rng.uniform(-45, 45)
        # We need every frame for audio recording, but not right now, so let's speed things up.
        self._skip_frames = 10
        self.turn_by(angle=angle)
        # Get the actual angle of the Magnebot (which won't be exactly the same as the target angle).
        y_rot = QuaternionUtils.get_y_angle(QuaternionUtils.IDENTITY, self.state.magnebot_transform.rotation)
        # Record the initialization pose.
        self._magnebot_init_data = MagnebotInitData(position=self.state.magnebot_transform.position, rotation=y_rot)
        # Stop skipping frames now that we're done turning.
        self._skip_frames = 0
        # Let the object fall.
        self.objects_static[self.target_object_id].kinematic = False
        self._next_frame_commands.extend([{"$type": "set_kinematic_state",
                                           "id": self.target_object_id,
                                           "is_kinematic": False,
                                           "use_gravity": True},
                                          {"$type": "set_object_collision_detection_mode",
                                           "id": self.target_object_id,
                                           "mode": "continuous_dynamic"}])
        # Reset the modes here to discard any junk generated during setup.
        Dataset.PY_IMPACT.reset(initial_amp=Dataset.INITIAL_AMP)
        return ActionStatus.success

    def _cache_static_data(self, resp: List[bytes]) -> None:
        super()._cache_static_data(resp=resp)
        self.env_id = self.get_unique_id()

    def _get_object_init_commands(self) -> List[dict]:
        # Get the commands for the scene objects.
        commands = get_object_init_commands(scene=self.scene, layout=self.layout)
        return commands

    def _get_start_trial_commands(self) -> List[dict]:
        # Disable any graphics settings that could affect performance (because the camera is off). Set the reverb space.
        return [{"$type": "set_post_process",
                 "value": False},
                {"$type": "enable_reflection_probes",
                 "enable": False},
                {"$type": "set_reverb_space_simple",
                 "env_id": -1,
                 "reverb_floor_material": self.env_audio_materials.floor,
                 "reverb_ceiling_material": self.env_audio_materials.wall,
                 "reverb_front_wall_material": self.env_audio_materials.wall,
                 "reverb_back_wall_material": self.env_audio_materials.wall,
                 "reverb_left_wall_material": self.env_audio_materials.wall,
                 "reverb_right_wall_material": self.env_audio_materials.wall}]

    def _get_end_init_commands(self) -> List[dict]:
        # Add an audio sensor. Apply a force.
        return [{"$type": "add_environ_audio_sensor"},
                {"$type": "apply_force_to_object",
                 "id": self.target_object_id,
                 "force": self.trials[self.trial_count].force}]

    def _get_magnebot_position(self) -> np.array:
        # Get all free occupancy map positions.
        occupancy_positions: List[np.array] = list()
        target_object_position = TDWUtils.vector3_to_array(self.trials[self.trial_count].init_data.position)
        # Prevent the Magnebot from spawning at edges of the occupancy map.
        spawn_map = np.zeros_like(self.occupancy_map)
        spawn_map.fill(1)
        spawn_map[self.occupancy_map == 0] = 0
        conv = np.ones((3, 3))
        spawn_map = convolve2d(spawn_map, conv, mode="same", boundary="fill")
        for ix, iy in np.ndindex(spawn_map.shape):
            if spawn_map[ix][iy] != 0:
                continue
            px, pz = self.get_occupancy_position(ix, iy)
            occupancy_positions.append(np.array([px, 0, pz]))
        # Sort the occupancy map positions by distance to the target object.
        occupancy_positions = list(sorted(occupancy_positions,
                                          key=lambda p: np.linalg.norm(p - target_object_position)))
        # Get the latter half of the positions (the further positions).
        occupancy_positions = occupancy_positions[int(len(occupancy_positions) / 2.0):]
        # Pick a random position.
        self._magnebot_position = occupancy_positions[self._rng.randint(0, len(occupancy_positions))]
        return self._magnebot_position

    def _get_target_object(self) -> Optional[MultiModalObjectInitData]:
        return self.trials[self.trial_count].init_data

    def _listen_for_audio(self) -> None:
        """
        Source: https://stackoverflow.com/questions/892199/detect-record-audio-in-python

        Loop until audio stops playing.
        """

        threshold = 5
        chunk_size = 1024
        audio_format = pyaudio.paInt16
        rate = 44100
        stream = Dataset.PY_AUDIO.open(format=audio_format, channels=1, rate=rate,
                                       input=True, output=True,
                                       frames_per_buffer=chunk_size,
                                       input_device_index=self._device_index)
        audio = True
        try:
            # Periodically check to see if any audio is playing.
            while audio:
                snd_data = array('h', stream.read(chunk_size))
                audio = max(snd_data) > threshold
                sleep(0.1)
        finally:
            stream.stop_stream()
            stream.close()

    @staticmethod
    def _get_pyaudio_device_index() -> int:
        """
        Source: https://stackoverflow.com/questions/36894315/how-to-select-a-specific-input-device-with-pyaudio

        :return: The index of the system audio device in PyAudio.
        """

        info = Dataset.PY_AUDIO.get_host_api_info_by_index(0)
        num_devices = info.get('deviceCount')
        for i in range(0, num_devices):
            if Dataset.PY_AUDIO.get_device_info_by_host_api_device_index(0, i).get('maxInputChannels') > 0:
                device_name = Dataset.PY_AUDIO.get_device_info_by_host_api_device_index(0, i).get('name')
                if "Stereo Mix" in device_name:
                    return i
        raise Exception("Couldn't find a suitable audio device!")


if __name__ == "__main__":
    dataset_generator = Dataset()
    dataset_generator.run()
